{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import requests\n",
    "import torch\n",
    "import transformers\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display_markdown\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "\n",
    "# Sanity checks regarding GPU\n",
    "assert torch.cuda.is_available()\n",
    "assert torch.cuda.is_bf16_supported()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN: str = os.environ[\"HF_TOKEN\"]\n",
    "LLM_MODEL: str = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "EMBED_MODEL: str = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "device = f\"cuda:{torch.cuda.current_device()}\"\n",
    "\n",
    "memory = joblib.Memory(\".cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8ffdb5a0e847f7b6c8bcb093abc5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Use BitsAndBytes to for lower quantization to reduce LLM's footprint\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Picks model configuration for pretrained model provided by HuggingFace community\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Ensure and load LLM model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline requires a tokenizer that handles translating plaintext into tokens\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Tweaks to tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HuggingFace pipeline\n",
    "generate_text = transformers.pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    # do_sample=True,\n",
    "    # temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=8192,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1,  # without this output begins repeating\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get content from the EEML 2024 website\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    with open(tmpdir + \"/eeml.html\", mode=\"w\") as fp:\n",
    "        fp.write(requests.get(\"https://www.eeml.eu/\").text)\n",
    "\n",
    "    docs = BSHTMLLoader(tmpdir + \"/eeml.html\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into chunks for vector store and address limitation of LLM's context length\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20) # TODO: Chunk size could probably be smaller\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Initialize FAISS (Facebook AI Similarity Search) embeddings database.\n",
    "vectorstore = FAISS.from_documents(all_splits, embedding)\n",
    "retreiver = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a helpful AI QA assistant. When answering questions, use the context enclosed by triple backquotes if it is relevant.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Reply your answer in markdown format.\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=PROMPT_TEMPLATE.strip(),\n",
    ")\n",
    "\n",
    "# Construct complete LLM chain\n",
    "llm_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retreiver,\n",
    "    return_source_documents=False,\n",
    "    # TODO: I don't like how I inject prompt template, but I couldn't find other way.\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template},\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "def answer_question(question: str, history: dict[str] = None) -> str:\n",
    "    if history is None: # Currently I don't have context/history implemented\n",
    "        history = []\n",
    "\n",
    "    response = llm_chain.invoke({\"question\": question, \"chat_history\": history})\n",
    "    answer = response[\"answer\"].split(\"### Answer:\")[-1].strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The Eastern European Machine Learning (EEML) Summer School is an annual event focused on machine learning research and development. It brings together leading researchers, industry experts, and students to learn about the latest advancements in machine learning and related fields. The conference covers various topics such as basics of machine learning, multimodal learning, natural language processing, advanced deep learning architectures, generative models, reinforcement learning, and AI for science. Speakers include renowned researchers and professionals from top institutions and companies like Google DeepMind, New York University, McGill University, and the University of Cambridge. The event also offers opportunities for networking, collaboration, and learning through tutorials and workshops."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is the EEML conference about?\"\n",
    "display_markdown(answer_question(question), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The speakers for the Eastern European Machine Learning Summer School 2024 include Aleksandra Faust from Google DeepMind, Alfredo Canziani from New York University, Chris Dyer from Google DeepMind, Doina Precup from McGill University and Google DeepMind, Jovana Mitrović from Google DeepMind, Kyunghyun Cho from New York University, Martin Vechev from ETH Zürich, INSAIT, Michael Bronstein from the University of Oxford, Mihaela van der Schaar from the University of Cambridge, Nenad Tomašev from Google DeepMind, Razvan Pascanu from Google DeepMind, Sander Dieleman from Google DeepMind, Velibor Ilić from the Institute for AI Research and Development of Serbia, Vladimir Gligorijević from Genentech, Yee Whye Teh from the University of Oxford, Anastasija Ilić from Google DeepMind, Andreea Deac from Isomorphic Labs, Cristian Bodnar from Microsoft Research, Ioana Bica from Google DeepMind, Iulia Duță from the University of Cambridge, Matko Bošnjac from Google DeepMind, Ognjen Milinković from the University of Belgrade, Petar Veličković from Google DeepMind, and teaching assistants TBD. The organizing team includes Doina Precup from McGill University and Google DeepMind, Razvan Pascanu from Google DeepMind, Viorica Patraucean from Google DeepMind, Branislav Kisačanin from NVIDIA, Dubravko Ćulibrk from the Institute for AI Research and Development of Serbia, Matko Bošnjak from Google DeepMind, and Nemanja Rakićević from Google DeepMind. Technical support will be provided by Gabriel Marchidan from IasiAI and Feel IT Services. Partners include The Institute for Artificial Intelligence Research and Development of Serbia and various sponsors. For more information or to sponsor the event, contact contact@eeml.eu."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Who are the speakers?\"\n",
    "display_markdown(answer_question(question), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, Petar Veličković is involved in Eastern European Machine Learning Summer School 2023 as a speaker. His affiliation is with Google DeepMind and the University of Cambridge."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Is Petar Veličković involved and what is his affiliation?\"\n",
    "display_markdown(answer_question(question), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
